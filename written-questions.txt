Written Questions

Q1. Run the web crawler using the configurations located at src/main/config/written_question_1a.json and
    src/main/config/written_question_1b.json. The only difference between these configurations is that one always uses
    the sequential crawler and the other always uses the parallel crawler. Inspect the profile output in
    profileData.txt.

    If you are using a multi-processor computer, you should notice that SequentialWebCrawler#crawl and
    ParallelWebCrawler#crawl took about the same amount of time, but PageParserImpl#parse took much longer when run with
    the ParallelWebCrawler.

    Why did the parser take more time when run with ParallelWebCrawler?

    * Answer: ParallelWebCrawler visited more URLs while parser works sequentially so it took more time to parse a larger amount of visited URLs.

Q2. Your manager ran your crawler on her old personal computer, using the configurations from Q1, and she notices that
    the sequential crawler actually outperforms the parallel crawler. She would like to know why.

    (a) Suggest one reason why the sequential web crawler was able to read more web pages than the parallel crawler.
        (Hint: Try setting "parallelism" to 1 in the JSON configs to simulate your manager's computer.)

    * Answer: Old personal computer has single core CPU so the parallel crawler actually run on only one thread
    while other threads are still expensive to create and maintain. That's why the sequential crawler works better.

    (b) Suggest one scenario in which the parallel web crawler will almost certainly perform better than the sequential
        crawler. Why will it perform better?

    * Answer: If the computer has a multi-core processor, parallel web crawler can do multiple tasks in parallel on many threads,
    so it can be able to do more work in the same amount of time.

Q3. Analyze your method profiler through the lens of Aspect Oriented Programming, by answering the following questions:

    (a) What cross-cutting concern is being addressed by the com.udacity.webcrawler.profiler.Profiler class?
    * Answer: The profiler aggregates information about profiled method calls, and how long they took.
    (b) What are the join points of the Profiler in the web crawler program?
    * Answer: Join points of the Profiler in the web crawler program are crawl method of WebCrawler, which is @Profiled annotated.                                                                                 .

Q4. Identify three (3) different design patterns used in this project, and explain which interfaces, classes, and/or
    libraries use or implement those design patterns.

    For each pattern, name one thing about the pattern that you LIKED, and one thing you DISLIKED. If you did not like
    anything, you can name two things you disliked.

    * Answer:

    1. Dependency Injection: An example is WebCrawlerMain uses this pattern to inject WebCrawler and Profiler into it.
    This design pattern makes classes loosely coupled, easier to test, but it requires extra configuration to achieve that.

    2. Builder: The CrawlerConfiguration, CrawlResult are some examples of using builder pattern in this project to create objects
    without mixing up the order of parameters. However we need to write some extra code to build a Builder for each class.

    3. Strategy: With this pattern, we implement crawl method of Webcrawler interface in different strategies
    for SequentialWebCrawler and ParallelWebCrawler to solve the task in different ways.
    One small disadvantage is strategy pattern requires us to specify the right implementation when we want to to execute the method.


